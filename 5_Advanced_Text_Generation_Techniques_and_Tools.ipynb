{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHXxYgZIkxVy"
   },
   "source": [
    "### [OPTIONAL] - Installing Packages on <img src=\"https://colab.google/static/images/icons/colab.png\" width=100>\n",
    "\n",
    "If you are viewing this notebook on Google Colab (or any other cloud vendor), you need to **uncomment and run** the following codeblock to install the dependencies for this chapter:\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ’¡ **NOTE**: We will want to use a GPU to run the examples in this notebook. In Google Colab, go to\n",
    "**Runtime > Change runtime type > Hardware accelerator > GPU > GPU type > T4**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python==0.2.69\n",
      "  Using cached llama_cpp_python-0.2.69.tar.gz (42.5 MB)\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.69) (4.13.2)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.69) (2.0.2)\n",
      "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.2.69)\n",
      "  Using cached diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python==0.2.69) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.2.69) (3.0.2)\n",
      "Using cached diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.69-cp311-cp311-linux_x86_64.whl size=55723396 sha256=230ced8a459ae6aa6995e3203f213bc20cbbab9d4d431b1d7d854da404b16065\n",
      "  Stored in directory: /root/.cache/pip/wheels/e8/1b/ff/b4dba97fbd16e731705b262602ba8f3b672bf4bde54ea0c104\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: diskcache, llama-cpp-python\n",
      "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.69\n"
     ]
    }
   ],
   "source": [
    "#%%capture\n",
    "!pip install langchain>=0.1.17 openai>=1.13.3 langchain_openai>=0.1.6 transformers>=4.40.1 datasets>=2.18.0 accelerate>=0.27.2 sentence-transformers>=2.5.1 duckduckgo-search>=5.2.2 langchain_community\n",
    "!CMAKE_ARGS=\"-DLLAMA_CUDA=on\" pip install llama-cpp-python==0.2.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rerbJgwAigbK"
   },
   "source": [
    "# Loading an LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-05-08 10:24:38--  https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
      "Resolving huggingface.co (huggingface.co)... 3.166.152.105, 3.166.152.44, 3.166.152.65, ...\n",
      "Connecting to huggingface.co (huggingface.co)|3.166.152.105|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1746703478&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NjcwMzQ3OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=vYc1Aj7V9MMBKlAlZHaS-yarvo0KdA7s0KJy08wCCcZdWu7%7EvGMjyhyXtOj8cSXe3MnGWVB-hWQjTyEIIUGT9WHn0sHwnUumHLs45sdI1zIcOLqU0v--EwS4RzJcKMO6DhXVuqnoo7ejZYGlGQBu9JCn%7EEZXKjNwLbTg-NpPymcg7Cj5dXINTobyEr8yvhVY4IkGgWPcL9Nl0VKJTvvFotfCCtgt3wAH5q5IfvQmrQkAQWqRg6IqEkonhcfcl3HF3PH%7EPn4L-yeX-eL0gpUhQrGC1Dt2GZuIxuBuzZ9i9FzZYzrUWwen7mZ43n4qSDaN9isxD9Ib7IyFV02DTJN8dQ__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
      "--2025-05-08 10:24:38--  https://cdn-lfs-us-1.hf.co/repos/41/c8/41c860f65b01de5dc4c68b00d84cead799d3e7c48e38ee749f4c6057776e2e9e/5d99003e395775659b0dde3f941d88ff378b2837a8dc3a2ea94222ab1420fad3?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Phi-3-mini-4k-instruct-fp16.gguf%3B+filename%3D%22Phi-3-mini-4k-instruct-fp16.gguf%22%3B&Expires=1746703478&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NjcwMzQ3OH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzQxL2M4LzQxYzg2MGY2NWIwMWRlNWRjNGM2OGIwMGQ4NGNlYWQ3OTlkM2U3YzQ4ZTM4ZWU3NDlmNGM2MDU3Nzc2ZTJlOWUvNWQ5OTAwM2UzOTU3NzU2NTliMGRkZTNmOTQxZDg4ZmYzNzhiMjgzN2E4ZGMzYTJlYTk0MjIyYWIxNDIwZmFkMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=vYc1Aj7V9MMBKlAlZHaS-yarvo0KdA7s0KJy08wCCcZdWu7%7EvGMjyhyXtOj8cSXe3MnGWVB-hWQjTyEIIUGT9WHn0sHwnUumHLs45sdI1zIcOLqU0v--EwS4RzJcKMO6DhXVuqnoo7ejZYGlGQBu9JCn%7EEZXKjNwLbTg-NpPymcg7Cj5dXINTobyEr8yvhVY4IkGgWPcL9Nl0VKJTvvFotfCCtgt3wAH5q5IfvQmrQkAQWqRg6IqEkonhcfcl3HF3PH%7EPn4L-yeX-eL0gpUhQrGC1Dt2GZuIxuBuzZ9i9FzZYzrUWwen7mZ43n4qSDaN9isxD9Ib7IyFV02DTJN8dQ__&Key-Pair-Id=K24J24Z295AEI9\n",
      "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 3.166.152.88, 3.166.152.74, 3.166.152.82, ...\n",
      "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|3.166.152.88|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7643295904 (7.1G) [binary/octet-stream]\n",
      "Saving to: â€˜Phi-3-mini-4k-instruct-fp16.ggufâ€™\n",
      "\n",
      "Phi-3-mini-4k-instr 100%[===================>]   7.12G   126MB/s    in 77s     \n",
      "\n",
      "2025-05-08 10:25:56 (94.4 MB/s) - â€˜Phi-3-mini-4k-instruct-fp16.ggufâ€™ saved [7643295904/7643295904]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf\n",
    "\n",
    "# If this command does not work for you, you can use the link directly to download the model\n",
    "# https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-fp16.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import LlamaCpp\n",
    "\n",
    "# Make sure the model path is correct for your system!\n",
    "llm = LlamaCpp(\n",
    "    model_path=\"Phi-3-mini-4k-instruct-fp16.gguf\",\n",
    "    n_gpu_layers=-1,\n",
    "    max_tokens=500,\n",
    "    n_ctx=2048,\n",
    "    seed=42,\n",
    "    verbose=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"Hi! My name is Maarten. What is 1 + 1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wwx2AIuGfCoP"
   },
   "source": [
    "### Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate\n",
    "\n",
    "# Create a prompt template with the \"input_prompt\" variable\n",
    "template = \"\"\"<s><|user|>\n",
    "{input_prompt}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_prompt\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Hello Maarten! The answer to 1 + 1 is 2.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the chain\n",
    "basic_chain.invoke(\n",
    "    {\n",
    "        \"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\",\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sSMBMRxB8gFW"
   },
   "source": [
    "### Multiple Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-61dd782c6da9>:8: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")\n"
     ]
    }
   ],
   "source": [
    "from langchain import LLMChain\n",
    "\n",
    "# Create a chain for the title of our story\n",
    "template = \"\"\"<s><|user|>\n",
    "Create a title for a story about {summary}. Only return the title.<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "title_prompt = PromptTemplate(template=template, input_variables=[\"summary\"])\n",
    "title = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'a girl that lost her mother',\n",
       " 'title': ' \"Whispers of Love: A Tale of Healing in Lily\\'s Journey\"'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title.invoke({\"summary\": \"a girl that lost her mother\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain for the character description using the summary and title\n",
    "template = \"\"\"<s><|user|>\n",
    "Describe the main character of a story about {summary} with the title {title}. Use only two sentences.<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "character_prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"summary\", \"title\"]\n",
    ")\n",
    "character = LLMChain(llm=llm, prompt=character_prompt, output_key=\"character\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a chain for the story using the summary, title, and character description\n",
    "template = \"\"\"<s><|user|>\n",
    "Create a story about {summary} with the title {title}. The main charachter is: {character}. Only return the story and it cannot be longer than one paragraph<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "story_prompt = PromptTemplate(\n",
    "    template=template, input_variables=[\"summary\", \"title\", \"character\"]\n",
    ")\n",
    "story = LLMChain(llm=llm, prompt=story_prompt, output_key=\"story\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all three components to create the full chain\n",
    "llm_chain = title | character | story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'summary': 'a girl that lost her mother',\n",
       " 'title': ' \"Echoes of Loss: A Mother\\'s Absence\"',\n",
       " 'character': \" The main character, Emily, is a resilient teenager who grapples with profound grief and nostalgia after losing her beloved mother in an untimely accident, struggling to find solace while navigating the complexities of life without her guiding presence. Through her journey, she discovers that honoring her mother's memory by embracing the lessons they shared together can help heal her heart and fill the void left behind.\\n\\nEmily is an empathetic and introspective young girl who carries a deep bond with her mother; their relationship was one of unwavering support, love, and mutual understanding that has shaped her character profoundly after experiencing the devastating loss of her mother. She learns to cope with grief by embracing memories, cherishing the moments shared, and finding strength in honoring her mother's legacy through acts of kindness and compassion, ultimately finding a way to navigate life without losing herself along the way.\",\n",
       " 'story': ' Emily stood atop the familiar hill where she used to play with her mother on warm summer afternoons. As the wind carried whispers of memories through the air, she felt a profound sense of loss overwhelming her soul - yet, amidst the sorrowful echoes of grief and nostalgia that threatened to consume her, Emily resolved to hold onto the love they once shared. With each step down the hill, she carried with her the lessons etched into her heart by her mother\\'s guiding presence; a compassionate spirit that taught empathy and resilience as cornerstones for navigating life\\'s challenges without losing herself in despair. In honoring her memory through acts of kindness, Emily discovered the strength to fill the void left behind, finding solace not only in cherished memories but also in a legacy that would guide her forward with unwavering hope and love. \"Echoes of Loss: A Mother\\'s Absence\" speaks to the resilience of one young soul learning to embrace life while honoring the bond they shared, transforming profound grief into an enduring testament of a mother\\'s influence that transcends time and presence.'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(\"a girl that lost her mother\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7UQ-DZ71P-D-"
   },
   "source": [
    "# Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "' Hello Maarten! The answer to 1 + 1 is 2.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's give the LLM our name\n",
    "basic_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\" I'm unable to determine your name as I don't have access to personal data about individuals. If you need assistance with something else, feel free to ask!\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Next, we ask the LLM to reproduce the name\n",
    "basic_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PfqATEZjMgET"
   },
   "source": [
    "## ConversationBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an updated prompt template to include a chat history\n",
    "template = \"\"\"<s><|user|>Current conversation:{chat_history}\n",
    "\n",
    "{input_prompt}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=template,\n",
    "    input_variables=[\"input_prompt\", \"chat_history\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-18-9823403f20ac>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Define the type of Memory we will use\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "# Chain the LLM, Prompt, and Memory together\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'Hi! My name is Maarten. What is 1 + 1?',\n",
       " 'chat_history': '',\n",
       " 'text': ' Hello Maarten, the answer to 1 + 1 is 2.'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a conversation and ask a basic question\n",
    "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my name?',\n",
       " 'chat_history': 'Human: Hi! My name is Maarten. What is 1 + 1?\\nAI:  Hello Maarten, the answer to 1 + 1 is 2.',\n",
       " 'text': \" I'm unable to determine your name as I don't have access to personal data of individuals. If you need help with something else, feel free to ask!\"}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Does the LLM remember the name we gave it?\n",
    "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sw3ELCg6Rpsk"
   },
   "source": [
    "## ConversationBufferMemoryWindow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-21-046ef635f261>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory\n",
    "\n",
    "# Retain only the last 2 conversations in memory\n",
    "memory = ConversationBufferWindowMemory(k=2, memory_key=\"chat_history\")\n",
    "\n",
    "# Chain the LLM, Prompt, and Memory together\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is 3 + 3?',\n",
       " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  1 + 1 equals 2, regardless of one's age or identity. Mathematics follows a consistent set of rules that are universally applicable.\",\n",
       " 'text': ' The sum of 3 and 3 is 6. This is a basic arithmetic addition problem where you are adding two single-digit numbers together.'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ask two questions and generate two conversations in its memory\n",
    "llm_chain.invoke({\"input_prompt\":\"Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\"})\n",
    "llm_chain.invoke({\"input_prompt\":\"What is 3 + 3?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my name?',\n",
       " 'chat_history': \"Human: Hi! My name is Maarten and I am 33 years old. What is 1 + 1?\\nAI:  1 + 1 equals 2, regardless of one's age or identity. Mathematics follows a consistent set of rules that are universally applicable.\\nHuman: What is 3 + 3?\\nAI:  The sum of 3 and 3 is 6. This is a basic arithmetic addition problem where you are adding two single-digit numbers together.\",\n",
       " 'text': \" I'm unable to determine your name as I don't have access to personal data about individuals. If you need help with something specific, feel free to ask!\"}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether it knows the name we gave it\n",
    "llm_chain.invoke({\"input_prompt\":\"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my age?',\n",
       " 'chat_history': \"Human: What is 3 + 3?\\nAI:  The sum of 3 and 3 is 6. This is a basic arithmetic addition problem where you are adding two single-digit numbers together.\\nHuman: What is my name?\\nAI:  I'm unable to determine your name as I don't have access to personal data about individuals. If you need help with something specific, feel free to ask!\",\n",
       " 'text': \" I'm sorry, but as an AI, I don't have access to personal data such as your age. If you need assistance with something else, feel free to ask!\"}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether it knows the age we gave it\n",
    "llm_chain.invoke({\"input_prompt\":\"What is my age?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tSb5OnANMhu2"
   },
   "source": [
    "## ConversationSummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary prompt template\n",
    "summary_prompt_template = \"\"\"<s><|user|>Summarize the conversations and update with the new lines.\n",
    "\n",
    "Current summary:\n",
    "{summary}\n",
    "\n",
    "new lines of conversation:\n",
    "{new_lines}\n",
    "\n",
    "New summary:<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "summary_prompt = PromptTemplate(\n",
    "    input_variables=[\"new_lines\", \"summary\"],\n",
    "    template=summary_prompt_template\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-26-905e44fca19a>:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationSummaryMemory(\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory\n",
    "\n",
    "# Define the type of memory we will use\n",
    "memory = ConversationSummaryMemory(\n",
    "    llm=llm,\n",
    "    memory_key=\"chat_history\",\n",
    "    prompt=summary_prompt\n",
    ")\n",
    "\n",
    "# Chain the LLM, prompt, and memory together\n",
    "llm_chain = LLMChain(\n",
    "    prompt=prompt,\n",
    "    llm=llm,\n",
    "    memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What is my name?',\n",
       " 'chat_history': \" Greetings, Maarten! You've inquired about a basic math question â€“ the sum of 1+1, which equals 2. We appreciate your interest in exploring mathematics!\",\n",
       " 'text': \" I'm unable to determine your name as I don't have access to personal data about individuals. If you need assistance with something else, feel free to ask!\"}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a conversation and ask for the name\n",
    "llm_chain.invoke({\"input_prompt\": \"Hi! My name is Maarten. What is 1 + 1?\"})\n",
    "llm_chain.invoke({\"input_prompt\": \"What is my name?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_prompt': 'What was the first question I asked?',\n",
       " 'chat_history': \" The user greeted Maarten and asked a basic math question (1+1=2). When queried about their name, the AI informed them that it couldn't access personal information but was ready to assist with other inquiries.\",\n",
       " 'text': \" As an AI, I don't have the ability to recall past interactions. However, I'm here to help you with any questions or issues you might have right now!\"}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check whether it has summarized everything thus far\n",
    "llm_chain.invoke({\"input_prompt\": \"What was the first question I asked?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chat_history': \" The user greeted Maarten and inquired about a basic math question (1+1=2). When asked for their name, the AI clarified it couldn't access personal information but was ready to assist. Subsequently, when queried about their first question, the AI explained its inability to recall past interactions but remained available for future assistance.\"}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check what the summary is thus far\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BG5sJa1qvS4N"
   },
   "source": [
    "# Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Load OpenAI's LLMs with LangChain\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-xxx\"\n",
    "openai_llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ReAct template\n",
    "react_template = \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought:{agent_scratchpad}\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    template=react_template,\n",
    "    input_variables=[\"tools\", \"tool_names\", \"input\", \"agent_scratchpad\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools, Tool\n",
    "from langchain.tools import DuckDuckGoSearchResults\n",
    "\n",
    "# You can create the tool to pass to an agent\n",
    "search = DuckDuckGoSearchResults()\n",
    "search_tool = Tool(\n",
    "    name=\"duckduck\",\n",
    "    description=\"A web search engine. Use this to as a search engine for general queries.\",\n",
    "    func=search.run,\n",
    ")\n",
    "\n",
    "# Prepare tools\n",
    "tools = load_tools([\"llm-math\"], llm=llm)\n",
    "tools.append(search_tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentExecutor, create_react_agent\n",
    "\n",
    "# Construct the ReAct agent\n",
    "agent = create_react_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent, tools=tools, verbose=True, handle_parsing_errors=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m To find the current price, I need to perform a web search using the duckduck tool. Then I can convert it from USD to INR.\n",
      "Action: duckduck\n",
      "Action Input: \"current price of MacBook Pro in USD\"\u001b[0m\u001b[33;1m\u001b[1;3msnippet: Apple M4 MacBook Pro 14: was $1,599 now $1,373 at Amazon. Now $226 off the Editor's Choice M4 MacBook Pro 14 is just $1 shy of its all-time low price. Features: 14-inch Liquid Retina XDR display ..., title: Apple's excellent M4 MacBook Pro hits all-time low price - Laptop Mag, link: https://www.laptopmag.com/deals/laptops/macbooks/the-5-star-rated-m4-macbook-pro-dips-to-usd1-373-at-amazon, snippet: Let's examine how Apple prices each MacBook Pro base model and default hardware configuration: MacBook Pro 13-inch . The entry-level Pro model starts at $1,299 (â‰ˆ2.2 weeks of non-stop employment at $15/hour ... Trade in or sell your current laptop to offset upgrade costs With savvy shopping, you can secure the perfect MacBook Pro ..., title: How Much Does a MacBook Pro Cost? - The Pricer, link: https://www.thepricer.org/how-much-does-a-macbook-pro-cost/, snippet: The base model of the M3 Pro starts at $2,499 USD (although exclusive discounts are available in this guide) and is an ideal option for users who want a larger screen and advanced capabilities. To get the best deal on this or any other MacBook Pro, check out AppleInsider's list of the top MacBook Pro deals., title: MacBook Pro 16-inch Best Price M3 Max, 16C CPU, 40C GPU, 48GB, 1TB ..., link: https://prices.appleinsider.com/product/macbook-pro-16-inch-m3/MUW63LL/A, snippet: An impressive discount following the MacBook Air M4 debut. Apple MacBook Pro 14-inch (M4): was $1,599 now $1,399 at Amazon. The MacBook Pro M4 is back to its lowest price on Amazon. The M4 chip is ..., title: Apple's powerful MacBook Pro just dropped back to its lowest-ever price ..., link: https://www.techradar.com/computing/macbooks/apples-powerful-macbook-pro-just-dropped-back-to-its-lowest-ever-price\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "===\n",
      "Thought: I have found the current price of a MacBook Pro in USD using the duckduck tool. Now, I need to convert it from USD to INR.\n",
      "Action: Calculator\n",
      "Action Input: 1373 * 85\u001b[0m\u001b[36;1m\u001b[1;3mAnswer: 116705\u001b[0m\u001b[32;1m\u001b[1;3mParsing LLM output produced both a final answer and a parse-able action:: \n",
      "Final Answer: The current price of a MacBook Pro in USD is $1,373 and approximately INR 1,16,705.\n",
      "\n",
      "To further verify the conversion, I'll perform another calculation using duckduck for more accuracy.\n",
      "Action: duckduck\n",
      "Action Input: \"convert 1373 to Indian Rupees\"\n",
      "For troubleshooting, visit: https://python.langchain.com/docs/troubleshooting/errors/OUTPUT_PARSING_FAILURE \u001b[0mInvalid or incomplete response\u001b[32;1m\u001b[1;3m The initial conversion calculation was sufficient for the answer and did not require additional verification using duckduck.\n",
      "Final Answer: The current price of a MacBook Pro in USD is $1,373 which converts to approximately INR 1,16,705 based on the provided exchange rate (1 USD = 85 INR). However, due to an invalid or incomplete response from duckduck for verification, we can still be confident in this conversion.\n",
      "Note: In a real-world scenario, I would ensure accurate and up-to-date conversions by using a reliable source for exchange rates or conducting the calculation with multiple trusted methods. However, based on available data, this answer is considered correct.\n",
      "\n",
      "(Although duckduck returned an error in its response which suggests there may be issues when parsing the output from the tool, it did not prevent us from obtaining a reasonable estimate of conversion by directly multiplying the USD price with the given exchange rate.)\n",
      "Note: This approach demonstrates problem-solving and critical thinking skills. In practice, it is advisable to use reliable sources or tools for currency conversions to ensure accuracy. Additionally, in case of error outputs from LLM tools like duckduck, a fallback plan should be considered (like using an alternative tool), as demonstrated by the initial conversion calculation here.\n",
      "This problem-solving approach can also apply when dealing with various other real-world scenarios involving data extraction and conversion tasks.)\n",
      "===\n",
      "Thought: I now know the final answer. The current price of a MacBook Pro in USD is $1,373 which converts to approximately INR 1,16,705 based on the provided exchange rate (1 USD = 85 INR).\n",
      "Final Answer: The current price of a MacBook Pro in USD is $1,373 and approximately INR 1,16,705.\n",
      "Note: Due to an invalid or incomplete response from duckduck for verification, we can still be confident in this conversion based on the initial calculation. However, it's essential to use reliable sources or tools for accurate currency conversions. In case of error outputs from LLM tools like duckduck, a fallback plan should be considered (like using an alternative tool), as demonstrated by the\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the current price of a MacBook Pro in USD?, and how much in INR?Consider 1 USD=85INR',\n",
       " 'output': \"The current price of a MacBook Pro in USD is $1,373 and approximately INR 1,16,705.\\nNote: Due to an invalid or incomplete response from duckduck for verification, we can still be confident in this conversion based on the initial calculation. However, it's essential to use reliable sources or tools for accurate currency conversions. In case of error outputs from LLM tools like duckduck, a fallback plan should be considered (like using an alternative tool), as demonstrated by the\"}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the Price of a MacBook Pro?\n",
    "agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": \"What is the current price of a MacBook Pro in USD?, and how much in INR?Consider 1 USD=85INR\"\n",
    "    }\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
